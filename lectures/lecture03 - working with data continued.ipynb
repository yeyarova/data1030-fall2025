{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **loc and iloc :)**\n",
    "    - we will go through this again today\n",
    "- **should we just use pandas in this class or would using both/polars benefit us more in and out of the classroom?**\n",
    "    - I'd say learn both, that's the most benefitial for you out of classroom.\n",
    "    - As far as problem sets and quizzes are concerned, it's fine to use one of the packages.\n",
    "    - I'll mostly use pandas going forward.\n",
    "- **What are some ways to get more familiar with working with python?**\n",
    "    - easy leetcode problems are a good start to learn standard python (no packages)\n",
    "    - you will practice how to work with popular data science packages well enough during class and the problem sets I think\n",
    "- **need to review basic python functions, like skiprows**\n",
    "    - skiprows is an argument of the pd.read_csv function :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Lecture 3</center>\n",
    "## <center>Working with data (step 0) continued</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning objectives\n",
    "By the end of the lecture, you will be able to\n",
    "- filter columns\n",
    "- merge and append data frames\n",
    "- modify a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Pandas and Polars </center>\n",
    "\n",
    "- data are often distributed over multiple files/databases (e.g., csv and excel files, sql databases)\n",
    "- each file/database is read into a pandas dataframe - last lecture\n",
    "- you often need to filter dataframes by selecting specific rows - last lecture\n",
    "- you often need to filter dataframes by selecting specific columns - today\n",
    "- multiple dataframes need to be merged and appended - today\n",
    "- dataframes sometimes need to be modified - today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some notes and advice\n",
    "\n",
    "- **ALWAYS READ THE HELP OF THE METHODS/FUNCTIONS YOU USE!**\n",
    "- stackoverflow is your friend, use it! https://stackoverflow.com/\n",
    "- you can also use generative AI to help you fix bugs\n",
    "    - Gemini is supported by Brown OIT (see here https://go.brown.edu/gemini)\n",
    "- [here](https://docs.pola.rs/user-guide/migration/pandas/) is an excellet review of the syntax differences between pandas and polars\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning objectives</font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to</font>\n",
    "- **filter columns**\n",
    "- <font color='LIGHTGRAY'>merge and append data frames</font>\n",
    "- <font color='LIGHTGRAY'>modify a dataframe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
      "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
      "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
      "       'gross-income'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.read_csv('../data/adult_data.csv')\n",
    "\n",
    "columns =  df_pd.columns\n",
    "print(columns)\n",
    "\n",
    "# select columns by column name\n",
    "#print(df_pd[['age','hours-per-week']])\n",
    "#print(columns[[1,5,7]])\n",
    "#print(df_pd[columns[[1,5,7]]])\n",
    "\n",
    "# select columns by index using iloc\n",
    "#print(df_pd.iloc[:,3])\n",
    "\n",
    "# select columns by index - not standard python indexing\n",
    "#print(df_pd.iloc[:,[3,5,6]])\n",
    "\n",
    "# select columns by index -  standard python indexing\n",
    "#print(df_pd.iloc[:,::2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in c:\\users\\ярова\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.33.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Ярова\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install polars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'gross-income']\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "df_pl = pl.read_csv('../data/adult_data.csv')\n",
    "\n",
    "columns =  df_pl.columns\n",
    "print(columns)\n",
    "\n",
    "# select columns by column name\n",
    "#print(df_pl['age','hours-per-week']) \n",
    "#print(df_pl.select(['age','hours-per-week'])) # use .select if you know the column names. it's often more numerically efficient and can be executed parallel \n",
    "#print(columns[1:4]) # indices must be integers or slices\n",
    "#print(df_pl[columns[1:4]])\n",
    "\n",
    "# select columns by index, polars has no .iloc\n",
    "#print(df_pl[:,3])\n",
    "\n",
    "# select columns by index - not standard python indexing but it works\n",
    "#print(df_pl[:,[3,5,6]])\n",
    "\n",
    "# select columns by index -  standard python indexing\n",
    "#print(df_pl[:,::2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning objectives</font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>filter columns</font>\n",
    "- **merge and append data frames**\n",
    "- <font color='LIGHTGRAY'>modify a dataframe</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to merge dataframes in Pandas?\n",
    "\n",
    "Merge - info on data points are distributed in multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  col1 col2\n",
      "0  ID1     5    y\n",
      "1  ID2     8    j\n",
      "2  ID3     2    w\n",
      "3  ID4     6    b\n",
      "4  ID5     0    a\n",
      "5  ID6     2    b\n",
      "6  ID7     5    t\n",
      "     ID  col3 col2\n",
      "0   ID2    12    q\n",
      "1   ID5    76    u\n",
      "2   ID6    34    e\n",
      "3  ID10    98    l\n",
      "4  ID11    65    p\n"
     ]
    }
   ],
   "source": [
    "# We have two datasets from two hospitals\n",
    "\n",
    "hospital1 = {'ID':['ID1','ID2','ID3','ID4','ID5','ID6','ID7'],'col1':[5,8,2,6,0,2,5],'col2':['y','j','w','b','a','b','t']}\n",
    "df1 = pd.DataFrame(data=hospital1)\n",
    "print(df1)\n",
    "\n",
    "hospital2 = {'ID':['ID2','ID5','ID6','ID10','ID11'],'col3':[12,76,34,98,65],'col2':['q','u','e','l','p']}\n",
    "df2 = pd.DataFrame(data=hospital2)\n",
    "print(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  col1 col2_x  col3 col2_y\n",
      "0  ID1     5      y   NaN    NaN\n",
      "1  ID2     8      j  12.0      q\n",
      "2  ID3     2      w   NaN    NaN\n",
      "3  ID4     6      b   NaN    NaN\n",
      "4  ID5     0      a  76.0      u\n",
      "5  ID6     2      b  34.0      e\n",
      "6  ID7     5      t   NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "# we are interested in only patients from hospital1\n",
    "df_left = df1.merge(df2,how='left',on='ID') # IDs from the left dataframe (df1) are kept\n",
    "print(df_left)\n",
    "\n",
    "# we are interested in only patients from hospital2\n",
    "#df_right = df1.merge(df2,how='right',on='ID') # IDs from the right dataframe (df2) are kept\n",
    "#df_right = df2.merge(df1,how='left',on='ID')\n",
    "#print(df_right)\n",
    "\n",
    "# we are interested in patiens who were in both hospitals\n",
    "#df_inner = df1.merge(df2,how='inner',on='ID') # merging on IDs present in both dataframes\n",
    "#print(df_inner)\n",
    "\n",
    "# we are interested in all patients who visited at least one of the hospitals\n",
    "#df_outer = df1.merge(df2,how='outer',on='ID')  # merging on IDs present in any dataframe\n",
    "#print(df_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to append dataframes in pandas?\n",
    "\n",
    "Append - new data comes in over a period of time. E.g., one file per month/quarter/fiscal year etc.\n",
    "\n",
    "You want to combine these files into one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  col1 col2  col3\n",
      "0   ID1   5.0    y   NaN\n",
      "1   ID2   8.0    j   NaN\n",
      "2   ID3   2.0    w   NaN\n",
      "3   ID4   6.0    b   NaN\n",
      "4   ID5   0.0    a   NaN\n",
      "5   ID6   2.0    b   NaN\n",
      "6   ID7   5.0    t   NaN\n",
      "0   ID2   NaN    q  12.0\n",
      "1   ID5   NaN    u  76.0\n",
      "2   ID6   NaN    e  34.0\n",
      "3  ID10   NaN    l  98.0\n",
      "4  ID11   NaN    p  65.0\n"
     ]
    }
   ],
   "source": [
    "df_append = pd.concat([df1,df2]) # note that rows with ID2, ID5, and ID6  are duplicated! Indices are duplicated too.\n",
    "print(df_append)\n",
    "\n",
    "#df_append = pd.concat([df1,df2],ignore_index=True) # note that rows with ID2, ID5, and ID6  are duplicated! \n",
    "#print(df_append)\n",
    "\n",
    "# d3 = {'ID':['ID23','ID94','ID56','ID17'],'col1':['rt','h','st','ne'],'col2':[23,86,23,78]}\n",
    "# df3 = pd.DataFrame(data=d3)\n",
    "# print(df3)\n",
    "\n",
    "# df_append = pd.concat([df1,df2,df3],ignore_index=True) # multiple dataframes can be appended\n",
    "# print(df_append)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to merge/join dataframes in Polars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m hospital1 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID6\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID7\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m----> 2\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mhospital1)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#print(df1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hospital2 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID6\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID10\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID11\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol3\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m76\u001b[39m,\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m98\u001b[39m,\u001b[38;5;241m65\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "hospital1 = {'ID':['ID1','ID2','ID3','ID4','ID5','ID6','ID7'],'col1':[5,8,2,6,0,2,5],'col2':['y','j','w','b','a','b','t']}\n",
    "df1 = pl.DataFrame(data=hospital1)\n",
    "#print(df1)\n",
    "\n",
    "hospital2 = {'ID':['ID2','ID5','ID6','ID10','ID11'],'col3':[12,76,34,98,65],'col2':['q','u','e','l','p']}\n",
    "df2 = pl.DataFrame(data=hospital2)\n",
    "#print(df2)\n",
    "\n",
    "\n",
    "# left join\n",
    "#df_left = df1.join(df2,how='left',on='ID') # IDs from the left dataframe (df1) are kept\n",
    "#print(df_left)\n",
    "\n",
    "# right join\n",
    "#df_right = df1.join(df2,how='right',on='ID') # IDs from the right dataframe (df2) are kept\n",
    "#df_right = df2.join(df1,how='left',on='ID')\n",
    "#print(df_right)\n",
    "\n",
    "# inner join\n",
    "#df_inner = df1.join(df2,how='inner',on='ID') # merging on IDs present in both dataframes\n",
    "#print(df_inner)\n",
    "\n",
    "# outer join is called a full join\n",
    "#df_outer = df1.join(df2,how='full',on='ID')  # merging on IDs present in any dataframe\n",
    "#print(df_outer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18988\\3126431949.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdf_append\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_append\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdf_merge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_append\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'subject_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ярова\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6314\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6315\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6316\u001b[0m         ):\n\u001b[0;32m   6317\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_data_1 = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5'],\n",
    "        'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], \n",
    "        'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches']}\n",
    "\n",
    "\n",
    "raw_data_2 = {\n",
    "        'subject_id': ['6', '7', '8', '9', '10'],\n",
    "        'first_name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], \n",
    "        'last_name': ['Bonder', 'Black', 'Balwner', 'Brice', 'Btisan']}\n",
    "\n",
    "raw_data_3 = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5', '7', '8', '9', '10', '11'],\n",
    "        'test_id': [51, 15, 15, 61, 16, 14, 15, 1, 61, 16]}\n",
    "\n",
    "df1 = pd.DataFrame(raw_data_1)\n",
    "df2 = pd.DataFrame(raw_data_2)\n",
    "df3 = pd.DataFrame(raw_data_3)\n",
    "\n",
    "df_append = df1.append(df2)\n",
    "print(df_append)\n",
    "\n",
    "df_merge = df_append.merge(df3,how='left',on='subject_id')\n",
    "print(df_merge)\n",
    "print(df_merge.shape)\n",
    "# Create three data frames from raw_data_1, 2, and 3.\n",
    "# Append the first two data frames and assign it to df_append.\n",
    "# Merge the third data frame with df_append such that only subject_ids from df_append are present. \n",
    "# Assign the new data frame to df_merge. \n",
    "# How many rows and columns do we have in df_merge?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Always check that the resulting dataframe is what you wanted to end up with!\n",
    "- small toy datasets are ideal to test your code.\n",
    "\n",
    "### If you need to do a more complicated dataframe operations, check out pd.concat() and pl.concat()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning objectives</font>\n",
    "<font color='LIGHTGRAY'>By the end of the lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>filter columns</font>\n",
    "- <font color='LIGHTGRAY'>merge and append data frames</font>\n",
    "- **modify a dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not EVER overwrite the original data files!\n",
    "## Always save the modified dataframe to a new file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to modify the dataset?\n",
    "- feature engineering\n",
    "    - generating new features (adding new informative columns) can improve the performance of the ML model\n",
    "- fix dataset issues\n",
    "    - there might be data quality issues that need to be manually fixed\n",
    "    - typos, missing values, etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country gross-income  \n",
      "0          2174             0              40   United-States        <=50K  \n",
      "1             0             0              13   United-States        <=50K  \n",
      "2             0             0              40   United-States        <=50K  \n",
      "3             0             0              40   United-States        <=50K  \n",
      "4             0             0              40            Cuba        <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.read_csv('../data/adult_data.csv')\n",
    "print(df_pd.head())\n",
    "\n",
    "# let's generate a new feature\n",
    "# is immigrant? False (0) if the person's home country is the USA, True (1) otherwise\n",
    "# such a feature has only two categories but it might be pretty informative for some purposes\n",
    "df_pd['is immigrant'] = df_pd['native-country'] != ' United-States'\n",
    "\n",
    "#print(df_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
